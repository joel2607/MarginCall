{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import Sequence"
      ],
      "metadata": {
        "id": "ifqabf9vl4UN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. Load and Preprocess Data\n",
        "# -------------------------\n",
        "# Read the CSV file (adjust the file path as needed)\n",
        "# Expected columns: Date, Open, High, Low, Close, Adj Close, Volume\n",
        "df = pd.read_csv('/content/train.csv', parse_dates=['Timestamp'])\n",
        "\n",
        "# Ensure the data is sorted by time and set the Date as index\n",
        "df.sort_values('Timestamp', inplace=True)\n",
        "df.set_index('Timestamp', inplace=True)\n"
      ],
      "metadata": {
        "id": "HP6FhLZTltMO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEHTfivxnc6W",
        "outputId": "5f6ea9b9-291f-46ff-8328-65ad6ede6350"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Hour', 'Minute',\n",
            "       'DayOfWeek'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 2. Extract Time Features for One-Minute Data\n",
        "# -------------------------\n",
        "# Extract time components\n",
        "df['Hour'] = df.index.hour         # Hour (0 to 23)\n",
        "df['Minute'] = df.index.minute     # Minute (0 to 59)\n",
        "df['DayOfWeek'] = df.index.dayofweek  # Day of week (0=Monday, …, 6=Sunday)\n",
        "\n",
        "# Cyclical encoding for hour\n",
        "df['sin_hour'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
        "df['cos_hour'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
        "\n",
        "# Cyclical encoding for minute\n",
        "df['sin_minute'] = np.sin(2 * np.pi * df['Minute'] / 60)\n",
        "df['cos_minute'] = np.cos(2 * np.pi * df['Minute'] / 60)\n",
        "\n",
        "# Cyclical encoding for day of week\n",
        "df['sin_dow'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "df['cos_dow'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)"
      ],
      "metadata": {
        "id": "Wfp8Vs-ol6qe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uac6NjEKxcRk",
        "outputId": "bf95cc83-ad74-4c73-efd4-ad1bb6b17c00"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Hour', 'Minute', 'DayOfWeek',\n",
            "       'sin_hour', 'cos_hour', 'sin_minute', 'cos_minute', 'sin_dow',\n",
            "       'cos_dow'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 3. Define Features and Target\n",
        "# -------------------------\n",
        "# Select features: price/volume info plus encoded time features.\n",
        "# You can adjust the list of features as needed.\n",
        "features = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
        "            'sin_hour', 'cos_hour', 'sin_minute', 'cos_minute',\n",
        "            'sin_dow', 'cos_dow']\n",
        "\n",
        "# Create a binary target. For example:\n",
        "# 1 if next minute's Close is higher than current minute's Close, else 0.\n",
        "df['target'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
        "\n",
        "# Remove the last row (which has no next-minute target)\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "O2Zw1vDZl8ZA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4. Scale the Features\n",
        "# -------------------------\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "# scaled_features: shape (n_samples, n_features)\n"
      ],
      "metadata": {
        "id": "A7UVBVyOl-9q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 5. Create a Data Generator to Produce Sequences On-The-Fly\n",
        "# -------------------------\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, features_array, target_array, seq_length, batch_size=32, shuffle=True):\n",
        "        \"\"\"\n",
        "        Initializes the data generator.\n",
        "        :param features_array: numpy array with shape (n_samples, n_features)\n",
        "        :param target_array: numpy array with shape (n_samples,)\n",
        "        :param seq_length: Number of time steps per sequence.\n",
        "        :param batch_size: Batch size.\n",
        "        :param shuffle: Whether to shuffle indices after each epoch.\n",
        "        \"\"\"\n",
        "        self.features_array = features_array\n",
        "        self.target_array = target_array\n",
        "        self.seq_length = seq_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        # The maximum start index to generate a full sequence:\n",
        "        self.indices = np.arange(len(features_array) - seq_length)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of batches per epoch\n",
        "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        batch_indices = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "        X_batch = np.array([self.features_array[i:i+self.seq_length] for i in batch_indices])\n",
        "        y_batch = np.array([self.target_array[i+self.seq_length] for i in batch_indices])\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle indices after each epoch if required\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "# Define sequence length (e.g., use the past 60 minutes to predict the next minute)\n",
        "seq_length = 60\n",
        "batch_size = 32\n",
        "target_values = df['target'].values\n",
        "\n",
        "# Create an instance of the DataGenerator\n",
        "data_gen = DataGenerator(scaled_features, target_values, seq_length, batch_size=batch_size)\n",
        "\n",
        "# Optionally, check one batch's shapes:\n",
        "X_batch, y_batch = data_gen[0]\n",
        "print(f\"Batch X shape: {X_batch.shape}\")  # Expected: (batch_size, seq_length, n_features)\n",
        "print(f\"Batch y shape: {y_batch.shape}\")    # Expected: (batch_size,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoX8VV2inUDO",
        "outputId": "5589df89-beb1-4499-dbc7-0e731e512401"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch X shape: (32, 60, 11)\n",
            "Batch y shape: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 6. Build the LSTM Model\n",
        "# -------------------------\n",
        "n_features = len(features)\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(seq_length, n_features)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "II6yxaGZmA7q",
        "outputId": "d32d7660-a222-4768-e348-7846ab159f04"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │          \u001b[38;5;34m12,400\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m20,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,400</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,651\u001b[0m (127.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,651</span> (127.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,651\u001b[0m (127.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,651</span> (127.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 7. Train the Model Using the Generator\n",
        "# -------------------------\n",
        "#early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Here, for simplicity, we use the same generator for training and validation.\n",
        "# In practice, create a separate validation generator.\n",
        "history = model.fit(\n",
        "    data_gen,\n",
        "    epochs=50,\n",
        "    validation_data=data_gen,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l83Xv3WQnigr",
        "outputId": "94f8ee89-866e-4a71-e1bc-6178aaa8d974"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 14ms/step - accuracy: 0.7441 - loss: 0.5675 - val_accuracy: 0.7449 - val_loss: 0.5644\n",
            "Epoch 2/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 17ms/step - accuracy: 0.7440 - loss: 0.5655 - val_accuracy: 0.7449 - val_loss: 0.5635\n",
            "Epoch 3/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 17ms/step - accuracy: 0.7444 - loss: 0.5647 - val_accuracy: 0.7449 - val_loss: 0.5634\n",
            "Epoch 4/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 15ms/step - accuracy: 0.7442 - loss: 0.5647 - val_accuracy: 0.7449 - val_loss: 0.5638\n",
            "Epoch 5/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 15ms/step - accuracy: 0.7450 - loss: 0.5637 - val_accuracy: 0.7449 - val_loss: 0.5632\n",
            "Epoch 6/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 15ms/step - accuracy: 0.7443 - loss: 0.5645 - val_accuracy: 0.7449 - val_loss: 0.5634\n",
            "Epoch 7/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 15ms/step - accuracy: 0.7438 - loss: 0.5645 - val_accuracy: 0.7449 - val_loss: 0.5631\n",
            "Epoch 8/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 15ms/step - accuracy: 0.7436 - loss: 0.5649 - val_accuracy: 0.7449 - val_loss: 0.5631\n",
            "Epoch 9/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 17ms/step - accuracy: 0.7452 - loss: 0.5627 - val_accuracy: 0.7449 - val_loss: 0.5630\n",
            "Epoch 10/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 17ms/step - accuracy: 0.7458 - loss: 0.5625 - val_accuracy: 0.7449 - val_loss: 0.5633\n",
            "Epoch 11/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 17ms/step - accuracy: 0.7456 - loss: 0.5626 - val_accuracy: 0.7449 - val_loss: 0.5631\n",
            "Epoch 12/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 15ms/step - accuracy: 0.7441 - loss: 0.5641 - val_accuracy: 0.7449 - val_loss: 0.5630\n",
            "Epoch 13/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 15ms/step - accuracy: 0.7456 - loss: 0.5623 - val_accuracy: 0.7449 - val_loss: 0.5630\n",
            "Epoch 14/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 15ms/step - accuracy: 0.7456 - loss: 0.5624 - val_accuracy: 0.7449 - val_loss: 0.5631\n",
            "Epoch 15/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 15ms/step - accuracy: 0.7455 - loss: 0.5623 - val_accuracy: 0.7449 - val_loss: 0.5627\n",
            "Epoch 16/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 17ms/step - accuracy: 0.7441 - loss: 0.5639 - val_accuracy: 0.7449 - val_loss: 0.5631\n",
            "Epoch 17/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 15ms/step - accuracy: 0.7449 - loss: 0.5631 - val_accuracy: 0.7449 - val_loss: 0.5629\n",
            "Epoch 18/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 14ms/step - accuracy: 0.7441 - loss: 0.5640 - val_accuracy: 0.7449 - val_loss: 0.5627\n",
            "Epoch 19/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 15ms/step - accuracy: 0.7453 - loss: 0.5625 - val_accuracy: 0.7449 - val_loss: 0.5628\n",
            "Epoch 20/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 15ms/step - accuracy: 0.7452 - loss: 0.5628 - val_accuracy: 0.7449 - val_loss: 0.5627\n",
            "Epoch 21/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 15ms/step - accuracy: 0.7442 - loss: 0.5634 - val_accuracy: 0.7449 - val_loss: 0.5625\n",
            "Epoch 22/50\n",
            "\u001b[1m11941/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 15ms/step - accuracy: 0.7450 - loss: 0.5624 - val_accuracy: 0.7449 - val_loss: 0.5626\n",
            "Epoch 23/50\n",
            "\u001b[1m 8762/11941\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 10ms/step - accuracy: 0.7438 - loss: 0.5636"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 8. Evaluate the Model\n",
        "# -------------------------\n",
        "# Evaluate on the generator (or on a separate test generator if available)\n",
        "loss, accuracy = model.evaluate(data_gen)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "KmkOWFgDqMhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory where you want to save the model\n",
        "save_dir = 'saved_models'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Define the full path for the model file\n",
        "save_path = os.path.join(save_dir, 'my_model.keras')\n",
        "\n",
        "# Save the model in the native Keras format\n",
        "model.save(save_path)\n",
        "print(f\"Model saved to '{save_path}'.\")\n"
      ],
      "metadata": {
        "id": "rJguy1fLb_xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "sSVlT7_hdxZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. Load the Trained Model and Scaler\n",
        "# -------------------------\n",
        "# Load your trained model (update the path if needed)\n",
        "model = load_model('/content/saved_models/my_model.keras')\n",
        "\n",
        "# Load your scaler (if you saved it to disk, for example using pickle)\n",
        "# For demonstration, we assume the scaler is already in memory as 'scaler'\n",
        "# If you saved it, you might do something like:\n",
        "# import pickle\n",
        "# with open('scaler.pkl', 'rb') as f:\n",
        "#     scaler = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "_damhioQdtyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 2. Load and Preprocess Testing Data\n",
        "# -------------------------\n",
        "# Read the test CSV file (adjust the file name/path as needed)\n",
        "df_test = pd.read_csv('/content/test.csv', parse_dates=['Timestamp'])\n",
        "\n",
        "# Sort by date and set Date as the index\n",
        "df_test.sort_values('Timestamp', inplace=True)\n",
        "df_test.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Extract time features from the Date index\n",
        "df_test['Hour'] = df_test.index.hour         # Hour (0-23)\n",
        "df_test['Minute'] = df_test.index.minute     # Minute (0-59)\n",
        "df_test['DayOfWeek'] = df_test.index.dayofweek  # Day of week (0=Monday, ... 6=Sunday)\n",
        "\n",
        "# Cyclical encoding for hour\n",
        "df_test['sin_hour'] = np.sin(2 * np.pi * df_test['Hour'] / 24)\n",
        "df_test['cos_hour'] = np.cos(2 * np.pi * df_test['Hour'] / 24)\n",
        "\n",
        "# Cyclical encoding for minute\n",
        "df_test['sin_minute'] = np.sin(2 * np.pi * df_test['Minute'] / 60)\n",
        "df_test['cos_minute'] = np.cos(2 * np.pi * df_test['Minute'] / 60)\n",
        "\n",
        "# Cyclical encoding for day of week\n",
        "df_test['sin_dow'] = np.sin(2 * np.pi * df_test['DayOfWeek'] / 7)\n",
        "df_test['cos_dow'] = np.cos(2 * np.pi * df_test['DayOfWeek'] / 7)\n",
        "\n",
        "# Define the feature columns (should match the ones used during training)\n",
        "features = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
        "            'sin_hour', 'cos_hour', 'sin_minute', 'cos_minute',\n",
        "            'sin_dow', 'cos_dow']\n",
        "\n",
        "# Create the binary target\n",
        "df_test['target'] = (df_test['Close'].shift(-1) > df_test['Close']).astype(int)\n",
        "df_test.dropna(inplace=True)  # Remove the last row that doesn't have a target\n"
      ],
      "metadata": {
        "id": "WvbxSjYId-nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 3. Scale the Test Features\n",
        "# -------------------------\n",
        "# Note: Use the same scaler that was fit on the training data\n",
        "scaled_test = scaler.transform(df_test[features].values)\n"
      ],
      "metadata": {
        "id": "0iztPWnMfuYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4. Create Sequences for the LSTM\n",
        "# -------------------------\n",
        "def create_sequences(features_array, target_array, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features_array) - seq_length):\n",
        "        X.append(features_array[i:i+seq_length])\n",
        "        y.append(target_array[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Set the sequence length (must match what you used during training)\n",
        "seq_length = 60\n",
        "target_test = df_test['target'].values\n",
        "\n",
        "# Create sequences from the test data\n",
        "X_test, y_test = create_sequences(scaled_test, target_test, seq_length)\n"
      ],
      "metadata": {
        "id": "mSM-awBUfvuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 5. Evaluate the Model on the Test Data\n",
        "# -------------------------\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "mJNl7_T3fyPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 6. Generate Predictions and Print Detailed Metrics\n",
        "# -------------------------\n",
        "# Generate predicted probabilities\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to binary predictions (using a threshold of 0.5)\n",
        "y_pred = (y_pred_probs >= 0.4).astype(int).flatten()\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "XwcJS4qTf0eO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}